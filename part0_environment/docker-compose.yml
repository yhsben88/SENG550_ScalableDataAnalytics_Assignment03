version: "3.8"
services:
  redis:
    image: redis:alpine3.22
    container_name: assignment3_redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data

  spark:
    image: apache/spark:4.1.0-preview4-scala2.13-java21-python3-r-ubuntu
    container_name: assignment3_spark
    environment:
      - SPARK_MODE=master
    ports:
      - "8080:8080"
    command: /bin/bash -lc "sleep 10 && /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.master.Master"

  airflow:
    image: apache/airflow:slim-latest-python3.13
    container_name: assignment3_airflow
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    volumes:
      - ../:/opt/airflow/dags
    ports:
      - "8081:8080"
    entrypoint: >
      /bin/bash -c "airflow db upgrade &&
                    airflow users create -u admin -p admin -r Admin -e admin@example.com -f Admin || true &&
                    airflow webserver & airflow scheduler"
volumes:
  redis-data:
